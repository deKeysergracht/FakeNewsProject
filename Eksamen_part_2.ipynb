{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I part 2 starter vi ud med at indlæse vores 'train_set.csv', 'validation_set.csv' og 'test_set.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_set.csv')\n",
    "val_data = pd.read_csv('validation_set.csv')\n",
    "test_data = pd.read_csv('test_set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi vil nu konvertere 'type' kolonnen til en binær klassificering, hvor 1 er lig med alle de reliable news og 0 er lig med alle de fake news. Samtidig med dette opretter vi en baseline model, som vi har valgt til at være naive bayes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8293869346733669\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Indlæs træningsdata\n",
    "train_data = pd.read_csv('train_set.csv')\n",
    "\n",
    "# Erstat NaN-værdier i 'stemmed_content' med en tom streng\n",
    "train_data['stemmed_content'].fillna('', inplace=True)\n",
    "\n",
    "# Definer dine kategorier\n",
    "reliable_categories = ['reliable', 'political']\n",
    "fake_categories = ['fake', 'bias', 'conspiracy', 'hate', 'junksci', 'rumor', 'unreliable', 'satire']\n",
    "\n",
    "# Opret en funktion, der omdanner 'type' til binære værdier\n",
    "def map_to_binary(article_type):\n",
    "    if article_type in reliable_categories:\n",
    "        return 1  # 'reliable'\n",
    "    elif article_type in fake_categories:\n",
    "        return 0  # 'fake'\n",
    "    else:\n",
    "        return 0  # Sikrer, at funktionen returnerer 0 for alle andre tilfælde, hvilket eliminerer risikoen for NaN-værdier\n",
    "\n",
    "# Opdater 'type' kolonnen i 'train_data' med de binære værdier\n",
    "train_data['type'] = train_data['type'].apply(map_to_binary)\n",
    "\n",
    "# Nu er 'train_data['type']' klar til brug som 'y_train'\n",
    "y_train = train_data['type']\n",
    "X_train = train_data['stemmed_content']\n",
    "\n",
    "# Opret en pipeline med TF-IDF og Naive Bayes\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Træn modellen på træningsdata\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Her er den del hvor vi indlæser valideringsdata og forudsiger\n",
    "# Antager at du har en 'val_set.csv' som valideringsdatasæt\n",
    "val_data = pd.read_csv('validation_set.csv')\n",
    "val_data['stemmed_content'].fillna('', inplace=True)\n",
    "\n",
    "# Opdater 'type' kolonnen i 'val_data' med de binære værdier\n",
    "val_data['type'] = val_data['type'].apply(map_to_binary)\n",
    "\n",
    "X_val = val_data['stemmed_content']\n",
    "y_val = val_data['type']\n",
    "\n",
    "# Brug pipelinen til at forudsige på valideringsdata\n",
    "y_pred = pipeline.predict(X_val)\n",
    "\n",
    "# Beregn og udskriv præcision for valideringsdata\n",
    "val_accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation accuracy: {val_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beregner også F1_Score, Recall og Precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8771\n",
      "Recall: 0.6841\n",
      "F1-Score: 0.7686\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Beregn præcision, recall og F1-score\n",
    "precision = precision_score(y_val, y_pred)\n",
    "recall = recall_score(y_val, y_pred)\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iforhold til meta data overvejede vi domain men i og med vi tidligere fandt ud af at hver domain kun udgiver 1 type artikel mener vi at det vil \"overfitte\" vores model, hvor vores model bare genkender domain istedet for at forudsige. Vi valgte derfor i stedet at prøve med authors og for at om dette spillede nogen rolle. Det viser sig at accuracy stiger med ca. 10%, med author som meta-feature rammer vi nu en accuracy på 92,2 %. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.9220603015075377\n",
      "LogisticRegression MSE: 0.08537117598564649\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Antager at train_data og val_data allerede er indlæst\n",
    "\n",
    "# Forberedelse af data (Erstat NaN-værdier og anvend map_to_binary funktionen)\n",
    "train_data['stemmed_content'].fillna('', inplace=True)\n",
    "train_data['authors'].fillna('', inplace=True)\n",
    "val_data['stemmed_content'].fillna('', inplace=True)\n",
    "val_data['authors'].fillna('', inplace=True)\n",
    "\n",
    "y_train = train_data['type'].apply(map_to_binary)\n",
    "y_val = val_data['type'].apply(map_to_binary)\n",
    "\n",
    "# Forbered X_train og X_val som dictionaries til ColumnTransformer\n",
    "X_train = train_data[['stemmed_content', 'authors']]\n",
    "X_val = val_data[['stemmed_content', 'authors']]\n",
    "\n",
    "# Opret en ColumnTransformer til at forarbejde de forskellige kolonner\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', TfidfVectorizer(), 'stemmed_content'),\n",
    "        ('authors', OneHotEncoder(handle_unknown='ignore'), ['authors'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Opret en pipeline med preprocessor og Naive Bayes\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Træn modellen på træningsdata\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Forudsige på valideringsdata\n",
    "y_pred = pipeline.predict(X_val)\n",
    "\n",
    "# Beregn og udskriv præcision for valideringsdata\n",
    "val_accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "mse_logistic = mean_squared_error(y_val, y_pred_probs)\n",
    "print(f\"LogisticRegression MSE: {mse_logistic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeg starter med at indlæse filen fra exercise 2 - scraped_articles.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_articles = pd.read_csv('scraped_articles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da disse artikler ikke er blevet renset eller lavet stemming på endnu, vil vi starte med at gøre dette:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                            cleaned_text\n",
       "0     we know whats coming east ukraine braces for r...\n",
       "1     in eastern ukraine the tide of this war hasnt ...\n",
       "2     in order to preserve life and encirclement i h...\n",
       "3     fighting has been raging in ukraine for two ye...\n",
       "4     deadly explosions have rocked ukraines souther...\n",
       "...                                                 ...\n",
       "5762  indias capital delhi has banned motorbike taxi...\n",
       "5763  two years ago indian prime minister narendra m...\n",
       "5764  the discovery of two charred bodies in a burnt...\n",
       "5765  the judges of masterchef india have been criti...\n",
       "5766  ruchelle barrie remembers feeling out of place...\n",
       "\n",
       "[5767 rows x 1 columns]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):  # Tjekker for NaN værdier og konverterer dem til en tom streng\n",
    "        return ''\n",
    "    text = str(text).lower()  # Sikrer at tekst er en streng og konverterer til små bogstaver\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Erstatter al hvidplads med et enkelt mellemrum\n",
    "    text = re.sub(r'\\b\\d{4}-\\d{2}-\\d{2}\\b', '<DATE>', text)  # Erstatter datoer med pladsholder\n",
    "    text = re.sub(r'\\d+', '<NUM>', text)  # Erstatter tal med pladsholder\n",
    "    text = re.sub(r'\\S+@\\S+', '<EMAIL>', text)  # Erstatter e-mails med pladsholder\n",
    "    text = re.sub(r'https?://\\S+', '<URL>', text)  # Erstatter URL'er med pladsholder\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Fjerner tegnsætning\n",
    "    return text\n",
    "\n",
    "# Path to the scraped articles CSV file\n",
    "csv_file_path = 'scraped_articles.csv'  # Erstat med din faktiske filsti\n",
    "\n",
    "# Load the scraped articles data from the CSV file\n",
    "scraped_articles = pd.read_csv(csv_file_path, dtype=str)\n",
    "\n",
    "# Apply the cleaning function to the 'text' column which contains the article text\n",
    "scraped_articles['cleaned_text'] = scraped_articles['text'].apply(clean_text)\n",
    "\n",
    "# Keeping only the 'cleaned_text' column for further processing\n",
    "cleaned_scraped_articles = scraped_articles[['cleaned_text']]\n",
    "\n",
    "# Optional: Save the cleaned data to a new CSV file if needed\n",
    "# cleaned_scraped_articles.to_csv('path_to_your_file/cleaned_scraped_articles.csv', index=False)\n",
    "cleaned_scraped_articles.head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/bruger/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/bruger/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>stemmed_content</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>we know whats coming east ukraine braces for r...</td>\n",
       "      <td>know what come east ukrain brace russian advan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in eastern ukraine the tide of this war hasnt ...</td>\n",
       "      <td>eastern ukrain tide war hasnt chang come fast ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in order to preserve life and encirclement i h...</td>\n",
       "      <td>order preserv life encircl withdrawn unit avdi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fighting has been raging in ukraine for two ye...</td>\n",
       "      <td>fight rage ukrain two year sinc russia invas m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deadly explosions have rocked ukraines souther...</td>\n",
       "      <td>deadli explos rock ukrain southern port citi o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_text  \\\n",
       "0  we know whats coming east ukraine braces for r...   \n",
       "1  in eastern ukraine the tide of this war hasnt ...   \n",
       "2  in order to preserve life and encirclement i h...   \n",
       "3  fighting has been raging in ukraine for two ye...   \n",
       "4  deadly explosions have rocked ukraines souther...   \n",
       "\n",
       "                                     stemmed_content  type  \n",
       "0  know what come east ukrain brace russian advan...     1  \n",
       "1  eastern ukrain tide war hasnt chang come fast ...     1  \n",
       "2  order preserv life encircl withdrawn unit avdi...     1  \n",
       "3  fight rage ukrain two year sinc russia invas m...     1  \n",
       "4  deadli explos rock ukrain southern port citi o...     1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download stopwords from NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def remove_stopwords_and_stem(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    # Remove stop words and stem each word\n",
    "    return ' '.join([stemmer.stem(word) for word in words if word not in stop_words])\n",
    "\n",
    "# Apply the function to remove stop words and stem the words in the 'cleaned_text' column\n",
    "# Brug .loc til at undgå SettingWithCopyWarning\n",
    "cleaned_scraped_articles.loc[:, 'stemmed_content'] = cleaned_scraped_articles['cleaned_text'].apply(remove_stopwords_and_stem)\n",
    "\n",
    "# Vis de første par rækker for at bekræfte ændringerne\n",
    "cleaned_scraped_articles.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu har vi altså renset og stemmed vores scraped_articles.csv og lavet en ny kolonne i dataen, som hedder cleaned_stemmed_text. Det næste vi vil gøre er at oprette en kolonne 'type' som fortæller om artiklen er reliable eller fake. Her vil vi vurdere alle artiklerne til at være reliable, da de kommer fra bbc. Dette vil altså sige at der kommer til at stå et 1 til under 'type' for alle artiklerne, da det betyder reliable i vores binære klassificering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/78/tggs5x_100l2bhsl5ydp24mh0000gn/T/ipykernel_13165/2909232008.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_scraped_articles['type'] = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>stemmed_content</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>we know whats coming east ukraine braces for r...</td>\n",
       "      <td>know what come east ukrain brace russian advan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in eastern ukraine the tide of this war hasnt ...</td>\n",
       "      <td>eastern ukrain tide war hasnt chang come fast ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in order to preserve life and encirclement i h...</td>\n",
       "      <td>order preserv life encircl withdrawn unit avdi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fighting has been raging in ukraine for two ye...</td>\n",
       "      <td>fight rage ukrain two year sinc russia invas m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deadly explosions have rocked ukraines souther...</td>\n",
       "      <td>deadli explos rock ukrain southern port citi o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5762</th>\n",
       "      <td>indias capital delhi has banned motorbike taxi...</td>\n",
       "      <td>india capit delhi ban motorbik taxi road deal ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5763</th>\n",
       "      <td>two years ago indian prime minister narendra m...</td>\n",
       "      <td>two year ago indian prime minist narendra modi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5764</th>\n",
       "      <td>the discovery of two charred bodies in a burnt...</td>\n",
       "      <td>discoveri two char bodi burnt vehicl india har...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5765</th>\n",
       "      <td>the judges of masterchef india have been criti...</td>\n",
       "      <td>judg masterchef india criticis social media le...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5766</th>\n",
       "      <td>ruchelle barrie remembers feeling out of place...</td>\n",
       "      <td>ruchel barri rememb feel place school name fre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5767 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           cleaned_text  \\\n",
       "0     we know whats coming east ukraine braces for r...   \n",
       "1     in eastern ukraine the tide of this war hasnt ...   \n",
       "2     in order to preserve life and encirclement i h...   \n",
       "3     fighting has been raging in ukraine for two ye...   \n",
       "4     deadly explosions have rocked ukraines souther...   \n",
       "...                                                 ...   \n",
       "5762  indias capital delhi has banned motorbike taxi...   \n",
       "5763  two years ago indian prime minister narendra m...   \n",
       "5764  the discovery of two charred bodies in a burnt...   \n",
       "5765  the judges of masterchef india have been criti...   \n",
       "5766  ruchelle barrie remembers feeling out of place...   \n",
       "\n",
       "                                        stemmed_content  type  \n",
       "0     know what come east ukrain brace russian advan...     1  \n",
       "1     eastern ukrain tide war hasnt chang come fast ...     1  \n",
       "2     order preserv life encircl withdrawn unit avdi...     1  \n",
       "3     fight rage ukrain two year sinc russia invas m...     1  \n",
       "4     deadli explos rock ukrain southern port citi o...     1  \n",
       "...                                                 ...   ...  \n",
       "5762  india capit delhi ban motorbik taxi road deal ...     1  \n",
       "5763  two year ago indian prime minist narendra modi...     1  \n",
       "5764  discoveri two char bodi burnt vehicl india har...     1  \n",
       "5765  judg masterchef india criticis social media le...     1  \n",
       "5766  ruchel barri rememb feel place school name fre...     1  \n",
       "\n",
       "[5767 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_scraped_articles['type'] = 1\n",
    "\n",
    "cleaned_scraped_articles.to_csv('cleaned_scraped_articled')\n",
    "\n",
    "cleaned_scraped_articles\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu hvor vi har en kolonne med stemmed_content hvor alle er sat til reliable under type, vil vi sammensætte vores train_data og cleaned_scraped_articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>domain</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>scraped_at</th>\n",
       "      <th>inserted_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>meta_keywords</th>\n",
       "      <th>meta_description</th>\n",
       "      <th>tags</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stemmed_content</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NUM</td>\n",
       "      <td>NUMNUM</td>\n",
       "      <td>abovetopsecretcom</td>\n",
       "      <td>0</td>\n",
       "      <td>URL</td>\n",
       "      <td>geoengineering cost analysis you lolyes seen ...</td>\n",
       "      <td>NUMNUMNUMtNUMNUMNUMNUM</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>possible weather mod setup page NUM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>geoengineering cost analysis lolyes seen docum...</td>\n",
       "      <td>geoengin cost analysi loly seen documentwhat t...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NUM</td>\n",
       "      <td>NUMNUM</td>\n",
       "      <td>thelibertybeaconcom</td>\n",
       "      <td>0</td>\n",
       "      <td>URL</td>\n",
       "      <td>lawNUM philadelphia february NUM NUM NUMNUM pm...</td>\n",
       "      <td>NUMNUMNUMtNUMNUMNUMNUM</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>merck sued in philly over shingles vaccine inj...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>natural immunity chickenpox merck lawsuit vacc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lawNUM philadelphia february NUM NUM NUMNUM pm...</td>\n",
       "      <td>lawnum philadelphia februari num num numnum pm...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NUM</td>\n",
       "      <td>NUM</td>\n",
       "      <td>newswithviewscom</td>\n",
       "      <td>0</td>\n",
       "      <td>URL</td>\n",
       "      <td>additional titles our right to attack iraq by ...</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>devvy kidd  new gun bill in congress more unco...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>additional titles right attack iraq devvy kidd...</td>\n",
       "      <td>addit titl right attack iraq devvi kidd februa...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NUM</td>\n",
       "      <td>NUM</td>\n",
       "      <td>nytimescom</td>\n",
       "      <td>1</td>\n",
       "      <td>URL</td>\n",
       "      <td>i dont know who takes him said an eastern conf...</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>multiple nicknames dwindling openings</td>\n",
       "      <td>howard beck</td>\n",
       "      <td>NaN</td>\n",
       "      <td>oneal shaquille basketball free agents sports ...</td>\n",
       "      <td>shaquille oneal who once rumbled across the co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nytimes</td>\n",
       "      <td>dont know takes said eastern conference scout ...</td>\n",
       "      <td>dont know take said eastern confer scout cite ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NUM</td>\n",
       "      <td>NUM</td>\n",
       "      <td>nationalreviewcom</td>\n",
       "      <td>1</td>\n",
       "      <td>URL</td>\n",
       "      <td>plus one article on google plus thanks to ali ...</td>\n",
       "      <td>NUMNUMNUMtNUMNUMNUMNUM</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>iran news round up</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>national review national review online article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>plus one article google plus thanks ali alfone...</td>\n",
       "      <td>plu one articl googl plu thank ali alfoneh ass...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0      id               domain  type  url  \\\n",
       "0        NUM  NUMNUM    abovetopsecretcom     0  URL   \n",
       "1        NUM  NUMNUM  thelibertybeaconcom     0  URL   \n",
       "2        NUM     NUM     newswithviewscom     0  URL   \n",
       "3        NUM     NUM           nytimescom     1  URL   \n",
       "4        NUM     NUM    nationalreviewcom     1  URL   \n",
       "\n",
       "                                             content              scraped_at  \\\n",
       "0   geoengineering cost analysis you lolyes seen ...  NUMNUMNUMtNUMNUMNUMNUM   \n",
       "1  lawNUM philadelphia february NUM NUM NUMNUM pm...  NUMNUMNUMtNUMNUMNUMNUM   \n",
       "2  additional titles our right to attack iraq by ...       DATE NUMNUMNUMNUM   \n",
       "3  i dont know who takes him said an eastern conf...       DATE NUMNUMNUMNUM   \n",
       "4  plus one article on google plus thanks to ali ...  NUMNUMNUMtNUMNUMNUMNUM   \n",
       "\n",
       "         inserted_at         updated_at  \\\n",
       "0  DATE NUMNUMNUMNUM  DATE NUMNUMNUMNUM   \n",
       "1  DATE NUMNUMNUMNUM  DATE NUMNUMNUMNUM   \n",
       "2  DATE NUMNUMNUMNUM  DATE NUMNUMNUMNUM   \n",
       "3  DATE NUMNUMNUMNUM  DATE NUMNUMNUMNUM   \n",
       "4  DATE NUMNUMNUMNUM  DATE NUMNUMNUMNUM   \n",
       "\n",
       "                                               title      authors  keywords  \\\n",
       "0                possible weather mod setup page NUM          NaN       NaN   \n",
       "1  merck sued in philly over shingles vaccine inj...          NaN       NaN   \n",
       "2  devvy kidd  new gun bill in congress more unco...          NaN       NaN   \n",
       "3              multiple nicknames dwindling openings  howard beck       NaN   \n",
       "4                                 iran news round up          NaN       NaN   \n",
       "\n",
       "                                       meta_keywords  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3  oneal shaquille basketball free agents sports ...   \n",
       "4     national review national review online article   \n",
       "\n",
       "                                    meta_description  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3  shaquille oneal who once rumbled across the co...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                                tags  summary   source  \\\n",
       "0                                                NaN      NaN      NaN   \n",
       "1  natural immunity chickenpox merck lawsuit vacc...      NaN      NaN   \n",
       "2                                                NaN      NaN      NaN   \n",
       "3                                                NaN      NaN  nytimes   \n",
       "4                                                NaN      NaN      NaN   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  geoengineering cost analysis lolyes seen docum...   \n",
       "1  lawNUM philadelphia february NUM NUM NUMNUM pm...   \n",
       "2  additional titles right attack iraq devvy kidd...   \n",
       "3  dont know takes said eastern conference scout ...   \n",
       "4  plus one article google plus thanks ali alfone...   \n",
       "\n",
       "                                     stemmed_content cleaned_text  \n",
       "0  geoengin cost analysi loly seen documentwhat t...          NaN  \n",
       "1  lawnum philadelphia februari num num numnum pm...          NaN  \n",
       "2  addit titl right attack iraq devvi kidd februa...          NaN  \n",
       "3  dont know take said eastern confer scout cite ...          NaN  \n",
       "4  plu one articl googl plu thank ali alfoneh ass...          NaN  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_scraped_combined = pd.concat([train_data, cleaned_scraped_articles], ignore_index=True)\n",
    "\n",
    "# Vis de første par rækker af den nye kombinerede DataFrame for at bekræfte, at alt ser korrekt ud\n",
    "train_scraped_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scraped_combined.to_csv('train_scraped_combined.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu prøver vi at bruge vores simple baseline \"naive bayes\" og ser om de 5767 ekstra artikler kan øge præcisionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy with additional reliable data: 0.8320904522613065\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Træn modellen på den nye kombinerede træningsdata\n",
    "pipeline.fit(train_scraped_combined['stemmed_content'], train_scraped_combined['type'])\n",
    "\n",
    "# Valideringsdatasættet forbliver det samme som før\n",
    "# Husk at sikre, at 'stemmed_content' og 'type' kolonnerne er forberedt som før\n",
    "\n",
    "# Brug pipelinen til at forudsige på valideringsdata\n",
    "y_pred = pipeline.predict(X_val)\n",
    "\n",
    "# Beregn og udskriv præcision for valideringsdata\n",
    "val_accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation accuracy with additional reliable data: {val_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som vi kan se på resultatet er nøjagtigheden blevet øget med ca. 0,3% hvilket giver god mening efter som vi har tilføjet 5767 artikler til train_data som i forvejen bestod af 796000 artikler. Da det kun er 5767 artikler vi har tilføjet til train_data havde vi også forventet en meget lille forbedring i nøjagtigheden. Vi har altså kun øget træningssættet med 0,72% og dermed skulle det ikke forbedre nøjagtigheden vildt meget. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bruger vores advanced model her, som resulterede i en logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8791256281407035\n",
      "LogisticRegression MSE: 0.08537117598564649\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# Indlæs træningsdata\n",
    "\n",
    "# Erstat NaN-værdier i 'stemmed_content' med en tom streng\n",
    "train_data['stemmed_content'].fillna('', inplace=True)\n",
    "\n",
    "# Definer dine kategorier\n",
    "reliable_categories = ['reliable', 'political']\n",
    "fake_categories = ['fake', 'conspiracy', 'hate', 'junksci', 'rumor', 'unreliable', 'satire','bias']\n",
    "\n",
    "# Opret en funktion, der omdanner 'type' til binære værdier\n",
    "def map_to_binary(article_type):\n",
    "    if article_type in reliable_categories:\n",
    "        return 1  # 'reliable'\n",
    "    elif article_type in fake_categories:\n",
    "        return 0  # 'fake'\n",
    "    # Sikrer, at funktionen returnerer 0 for alle andre tilfælde, hvilket eliminerer risikoen for NaN-værdier\n",
    "    return 0  \n",
    "\n",
    "# Anvend denne funktion på dit datasæt for at oprette en binær label\n",
    "y_train = train_data['type'].apply(map_to_binary)\n",
    "\n",
    "# Nu er 'y_train' klar uden risiko for NaN-værdier\n",
    "X_train = train_data['stemmed_content']  # 'X_train' er også klar til brug\n",
    "\n",
    "# Opret en pipeline med TF-IDF og logistisk regression, øg antallet af iterationer\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('classifier', LogisticRegression(max_iter=10000))  \n",
    "])\n",
    "\n",
    "# Træn modellen på træningsdata\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Her er den del hvor vi indlæser valideringsdata og forudsiger\n",
    "# Antager at du har en 'val_set.csv' som valideringsdatasæt\n",
    "val_data['stemmed_content'].fillna('', inplace=True)\n",
    "X_val = val_data['stemmed_content']\n",
    "y_val = val_data['type'].apply(map_to_binary)\n",
    "\n",
    "# Brug pipeline til at forudsige på valideringsdata\n",
    "y_pred = pipeline.predict(X_val)\n",
    "\n",
    "# Forudsagte sandsynligheder (hvis du har brug for dem til MSE beregning)\n",
    "y_pred_probs = pipeline.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Beregn og udskriv præcision for valideringsdata\n",
    "val_accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "# Beregn MSE ved at sammenligne forudsagte sandsynligheder med de faktiske labels\n",
    "# Bemærk: Dette er ikke standard for klassificering og anvendes normalt i regression.\n",
    "mse_logistic = mean_squared_error(y_val, y_pred_probs)\n",
    "print(f\"LogisticRegression MSE: {mse_logistic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Beregn præcision, recall og F1-score\n",
    "precision = precision_score(y_val, y_pred)\n",
    "recall = recall_score(y_val, y_pred)\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her tilføjer vi authors som en meta feature og kan se at modellen virker bedre nu og giver en bedre accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Antager at train_data og val_data allerede er indlæst\n",
    "\n",
    "# Erstat NaN-værdier i 'stemmed_content' og 'domain' med en tom streng\n",
    "train_data['stemmed_content'].fillna('', inplace=True)\n",
    "train_data['authors'].fillna('', inplace=True)\n",
    "\n",
    "val_data['stemmed_content'].fillna('', inplace=True)\n",
    "val_data['authors'].fillna('', inplace=True)\n",
    "\n",
    "# Definer dine kategorier\n",
    "reliable_categories = ['reliable', 'political']\n",
    "fake_categories = ['fake', 'bias', 'conspiracy', 'hate', 'junksci', 'rumor', 'unreliable', 'satire']\n",
    "\n",
    "# Opret en funktion, der omdanner 'type' til binære værdier\n",
    "def map_to_binary(article_type):\n",
    "    return 1 if article_type in reliable_categories else 0\n",
    "\n",
    "# Anvend denne funktion på dit datasæt for at oprette en binær label\n",
    "y_train = train_data['type'].apply(map_to_binary)\n",
    "y_val = val_data['type'].apply(map_to_binary)\n",
    "\n",
    "# Forbered features\n",
    "X_train = train_data[['stemmed_content', 'authors']]\n",
    "X_val = val_data[['stemmed_content', 'authors']]\n",
    "\n",
    "# Opret en ColumnTransformer til at forarbejde de forskellige kolonner\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', TfidfVectorizer(), 'stemmed_content'),\n",
    "        ('authors', OneHotEncoder(handle_unknown='ignore'), ['authors'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Opret en pipeline med preprocessor og logistisk regression\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Træn modellen på træningsdata\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Forudsige på valideringsdata\n",
    "y_pred = pipeline.predict(X_val)\n",
    "\n",
    "# Beregn og udskriv præcision for valideringsdata\n",
    "val_accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "# Beregn MSE ved at sammenligne forudsagte sandsynligheder med de faktiske labels\n",
    "y_pred_probs = pipeline.predict_proba(X_val)[:, 1]\n",
    "mse_logistic = mean_squared_error(y_val, y_pred_probs)\n",
    "print(f\"LogisticRegression MSE: {mse_logistic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Beregn præcision, recall og F1-score\n",
    "precision = precision_score(y_val, y_pred)\n",
    "recall = recall_score(y_val, y_pred)\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
