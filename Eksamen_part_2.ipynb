{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I part 2 starter vi ud med at indlæse vores 'train_set.csv', 'validation_set.csv' og 'test_set.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_set.csv')\n",
    "val_data = pd.read_csv('validation_set.csv')\n",
    "test_data = pd.read_csv('test_set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu konvertere 'type' kolonnen til en binær klassificering, hvor 1 er lig med reliable news og 0 er lig fake news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definer dine kategorier\n",
    "reliable_categories = ['reliable', 'political']\n",
    "fake_categories = ['fake', 'bias', 'conspiracy', 'hate', 'junksci', 'rumor', 'unreliable', 'satire']\n",
    "\n",
    "# Opret en funktion, der omdanner 'type' til binære værdier\n",
    "def map_to_binary(article_type):\n",
    "    if article_type in reliable_categories:\n",
    "        return 1  # 'reliable'\n",
    "    elif article_type in fake_categories:\n",
    "        return 0  # 'fake'\n",
    "    else:\n",
    "        return 0  # Sikrer, at funktionen returnerer 0 for alle andre tilfælde, hvilket eliminerer risikoen for NaN-værdier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her opretter vi vores baseline model ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8295879396984924\n",
      "F1-Score: 0.7692\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "# Indlæs træningsdata\n",
    "train_data = pd.read_csv('train_set.csv')\n",
    "\n",
    "# Erstat NaN-værdier i 'stemmed_content' med en tom streng\n",
    "train_data['stemmed_content'].fillna('', inplace=True)\n",
    "\n",
    "# Opdater 'type' kolonnen i 'train_data' med de binære værdier\n",
    "train_data['type'] = train_data['type'].apply(map_to_binary)\n",
    "\n",
    "# Nu er 'train_data['type']' klar til brug som 'y_train'\n",
    "y_train = train_data['type']\n",
    "X_train = train_data['stemmed_content']\n",
    "\n",
    "# Opret en pipeline med TF-IDF og Naive Bayes\n",
    "baseline_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Træn modellen på træningsdata\n",
    "baseline_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Her er den del hvor vi indlæser valideringsdata og forudsiger\n",
    "# Antager at du har en 'val_set.csv' som valideringsdatasæt\n",
    "val_data = pd.read_csv('validation_set.csv')\n",
    "val_data['stemmed_content'].fillna('', inplace=True)\n",
    "\n",
    "# Opdater 'type' kolonnen i 'val_data' med de binære værdier\n",
    "val_data['type'] = val_data['type'].apply(map_to_binary)\n",
    "\n",
    "X_val = val_data['stemmed_content']\n",
    "y_val = val_data['type']\n",
    "\n",
    "# Brug pipelinen til at forudsige på valideringsdata\n",
    "y_pred = baseline_pipeline.predict(X_val)\n",
    "\n",
    "# Beregn og udskriv præcision for valideringsdata\n",
    "val_accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation accuracy: {val_accuracy}\")\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "print(f\"F1-Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iforhold til meta data overvejede vi domain men i og med vi tidligere fandt ud af at hver domain kun udgiver 1 type artikel mener vi at det vil \"overfitte\" vores model, hvor vores model bare genkender domain istedet for at forudsige. Vi valgte i stedet at prøve med authors, hvor det viser sig at accuracy stiger med ca. 10%, med author som meta-feature rammer vi nu en accuracy på 92,2 %. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.9213668341708543\n",
      "F1-Score: 0.8985\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Indlæs træningsdata\n",
    "train_data = pd.read_csv('train_set.csv')\n",
    "train_data['stemmed_content'].fillna('', inplace=True)\n",
    "train_data['authors'].fillna('', inplace=True)\n",
    "\n",
    "# Indlæs valideringsdata\n",
    "val_data = pd.read_csv('validation_set.csv')\n",
    "val_data['stemmed_content'].fillna('', inplace=True)\n",
    "val_data['authors'].fillna('', inplace=True)\n",
    "\n",
    "# Omdan 'type' til binære værdier\n",
    "train_data['type'] = train_data['type'].apply(map_to_binary)\n",
    "val_data['type'] = val_data['type'].apply(map_to_binary)\n",
    "\n",
    "# Forbered features og labels\n",
    "X_train = train_data[['stemmed_content', 'authors']]\n",
    "y_train = train_data['type']\n",
    "X_val = val_data[['stemmed_content', 'authors']]\n",
    "y_val = val_data['type']\n",
    "\n",
    "# Opret en ColumnTransformer til at forarbejde de forskellige kolonner\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', TfidfVectorizer(), 'stemmed_content'),\n",
    "        ('authors', OneHotEncoder(handle_unknown='ignore'), ['authors'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Opret en pipeline med preprocessor og Naive Bayes\n",
    "baseline_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Træn modellen på træningsdata\n",
    "baseline_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Forudsige på valideringsdata\n",
    "y_pred = baseline_pipeline.predict(X_val)\n",
    "\n",
    "# Beregn og udskriv præcision for valideringsdata\n",
    "val_accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indlæser filen fra exercise 2 - scraped_articles.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_articles = pd.read_csv('scraped_articles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da disse artikler ikke er blevet renset eller lavet stemming på endnu, vil vi starte med at gøre dette:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                            cleaned_text\n",
       "0     we know whats coming east ukraine braces for r...\n",
       "1     in eastern ukraine the tide of this war hasnt ...\n",
       "2     in order to preserve life and encirclement i h...\n",
       "3     fighting has been raging in ukraine for two ye...\n",
       "4     deadly explosions have rocked ukraines souther...\n",
       "...                                                 ...\n",
       "5762  indias capital delhi has banned motorbike taxi...\n",
       "5763  two years ago indian prime minister narendra m...\n",
       "5764  the discovery of two charred bodies in a burnt...\n",
       "5765  the judges of masterchef india have been criti...\n",
       "5766  ruchelle barrie remembers feeling out of place...\n",
       "\n",
       "[5767 rows x 1 columns]>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):  # Tjekker for NaN værdier og konverterer dem til en tom streng\n",
    "        return ''\n",
    "    text = str(text).lower()  # Sikrer at tekst er en streng og konverterer til små bogstaver\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Erstatter al hvidplads med et enkelt mellemrum\n",
    "    text = re.sub(r'\\b\\d{4}-\\d{2}-\\d{2}\\b', '<DATE>', text)  # Erstatter datoer med pladsholder\n",
    "    text = re.sub(r'\\d+', '<NUM>', text)  # Erstatter tal med pladsholder\n",
    "    text = re.sub(r'\\S+@\\S+', '<EMAIL>', text)  # Erstatter e-mails med pladsholder\n",
    "    text = re.sub(r'https?://\\S+', '<URL>', text)  # Erstatter URL'er med pladsholder\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Fjerner tegnsætning\n",
    "    return text\n",
    "\n",
    "# Path to the scraped articles CSV file\n",
    "csv_file_path = 'scraped_articles.csv'  # Erstat med din faktiske filsti\n",
    "\n",
    "# Load the scraped articles data from the CSV file\n",
    "scraped_articles = pd.read_csv(csv_file_path, dtype=str)\n",
    "\n",
    "# Apply the cleaning function to the 'text' column which contains the article text\n",
    "scraped_articles['cleaned_text'] = scraped_articles['text'].apply(clean_text)\n",
    "\n",
    "# Keeping only the 'cleaned_text' column for further processing\n",
    "cleaned_scraped_articles = scraped_articles[['cleaned_text']]\n",
    "\n",
    "# Optional: Save the cleaned data to a new CSV file if needed\n",
    "# cleaned_scraped_articles.to_csv('path_to_your_file/cleaned_scraped_articles.csv', index=False)\n",
    "cleaned_scraped_articles.head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/adamjohn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/adamjohn/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/var/folders/kl/5qdj5c_s0fs5_ym1bdzn3k1c0000gn/T/ipykernel_56807/1439455173.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_scraped_articles.loc[:, 'stemmed_content'] = cleaned_scraped_articles['cleaned_text'].apply(remove_stopwords_and_stem)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>stemmed_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>we know whats coming east ukraine braces for r...</td>\n",
       "      <td>know what come east ukrain brace russian advan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in eastern ukraine the tide of this war hasnt ...</td>\n",
       "      <td>eastern ukrain tide war hasnt chang come fast ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in order to preserve life and encirclement i h...</td>\n",
       "      <td>order preserv life encircl withdrawn unit avdi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fighting has been raging in ukraine for two ye...</td>\n",
       "      <td>fight rage ukrain two year sinc russia invas m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deadly explosions have rocked ukraines souther...</td>\n",
       "      <td>deadli explos rock ukrain southern port citi o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_text  \\\n",
       "0  we know whats coming east ukraine braces for r...   \n",
       "1  in eastern ukraine the tide of this war hasnt ...   \n",
       "2  in order to preserve life and encirclement i h...   \n",
       "3  fighting has been raging in ukraine for two ye...   \n",
       "4  deadly explosions have rocked ukraines souther...   \n",
       "\n",
       "                                     stemmed_content  \n",
       "0  know what come east ukrain brace russian advan...  \n",
       "1  eastern ukrain tide war hasnt chang come fast ...  \n",
       "2  order preserv life encircl withdrawn unit avdi...  \n",
       "3  fight rage ukrain two year sinc russia invas m...  \n",
       "4  deadli explos rock ukrain southern port citi o...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download stopwords from NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def remove_stopwords_and_stem(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    # Remove stop words and stem each word\n",
    "    return ' '.join([stemmer.stem(word) for word in words if word not in stop_words])\n",
    "\n",
    "# Apply the function to remove stop words and stem the words in the 'cleaned_text' column\n",
    "# Brug .loc til at undgå SettingWithCopyWarning\n",
    "cleaned_scraped_articles.loc[:, 'stemmed_content'] = cleaned_scraped_articles['cleaned_text'].apply(remove_stopwords_and_stem)\n",
    "\n",
    "# Vis de første par rækker for at bekræfte ændringerne\n",
    "cleaned_scraped_articles.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu har vi altså renset og stemmed vores scraped_articles.csv og lavet en ny kolonne i dataen, som hedder cleaned_stemmed_text. Det næste vi vil gøre er at oprette en kolonne 'type' som fortæller om artiklen er reliable eller fake. Her vil vi vurdere alle artiklerne til at være reliable, da de kommer fra bbc. Dette vil altså sige at der kommer til at stå et 1 til under 'type' for alle artiklerne, da det betyder reliable i vores binære klassificering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kl/5qdj5c_s0fs5_ym1bdzn3k1c0000gn/T/ipykernel_56807/2909232008.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_scraped_articles['type'] = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>stemmed_content</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>we know whats coming east ukraine braces for r...</td>\n",
       "      <td>know what come east ukrain brace russian advan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in eastern ukraine the tide of this war hasnt ...</td>\n",
       "      <td>eastern ukrain tide war hasnt chang come fast ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in order to preserve life and encirclement i h...</td>\n",
       "      <td>order preserv life encircl withdrawn unit avdi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fighting has been raging in ukraine for two ye...</td>\n",
       "      <td>fight rage ukrain two year sinc russia invas m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deadly explosions have rocked ukraines souther...</td>\n",
       "      <td>deadli explos rock ukrain southern port citi o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5762</th>\n",
       "      <td>indias capital delhi has banned motorbike taxi...</td>\n",
       "      <td>india capit delhi ban motorbik taxi road deal ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5763</th>\n",
       "      <td>two years ago indian prime minister narendra m...</td>\n",
       "      <td>two year ago indian prime minist narendra modi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5764</th>\n",
       "      <td>the discovery of two charred bodies in a burnt...</td>\n",
       "      <td>discoveri two char bodi burnt vehicl india har...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5765</th>\n",
       "      <td>the judges of masterchef india have been criti...</td>\n",
       "      <td>judg masterchef india criticis social media le...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5766</th>\n",
       "      <td>ruchelle barrie remembers feeling out of place...</td>\n",
       "      <td>ruchel barri rememb feel place school name fre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5767 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           cleaned_text  \\\n",
       "0     we know whats coming east ukraine braces for r...   \n",
       "1     in eastern ukraine the tide of this war hasnt ...   \n",
       "2     in order to preserve life and encirclement i h...   \n",
       "3     fighting has been raging in ukraine for two ye...   \n",
       "4     deadly explosions have rocked ukraines souther...   \n",
       "...                                                 ...   \n",
       "5762  indias capital delhi has banned motorbike taxi...   \n",
       "5763  two years ago indian prime minister narendra m...   \n",
       "5764  the discovery of two charred bodies in a burnt...   \n",
       "5765  the judges of masterchef india have been criti...   \n",
       "5766  ruchelle barrie remembers feeling out of place...   \n",
       "\n",
       "                                        stemmed_content  type  \n",
       "0     know what come east ukrain brace russian advan...     1  \n",
       "1     eastern ukrain tide war hasnt chang come fast ...     1  \n",
       "2     order preserv life encircl withdrawn unit avdi...     1  \n",
       "3     fight rage ukrain two year sinc russia invas m...     1  \n",
       "4     deadli explos rock ukrain southern port citi o...     1  \n",
       "...                                                 ...   ...  \n",
       "5762  india capit delhi ban motorbik taxi road deal ...     1  \n",
       "5763  two year ago indian prime minist narendra modi...     1  \n",
       "5764  discoveri two char bodi burnt vehicl india har...     1  \n",
       "5765  judg masterchef india criticis social media le...     1  \n",
       "5766  ruchel barri rememb feel place school name fre...     1  \n",
       "\n",
       "[5767 rows x 3 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_scraped_articles['type'] = 1\n",
    "\n",
    "cleaned_scraped_articles.to_csv('cleaned_scraped_articled')\n",
    "\n",
    "cleaned_scraped_articles\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu hvor vi har en kolonne med stemmed_content hvor alle er sat til reliable under type, vil vi sammensætte vores train_data og cleaned_scraped_articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>domain</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>scraped_at</th>\n",
       "      <th>inserted_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>meta_keywords</th>\n",
       "      <th>meta_description</th>\n",
       "      <th>tags</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stemmed_content</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NUM</td>\n",
       "      <td>NUM</td>\n",
       "      <td>pravdareportcom</td>\n",
       "      <td>0</td>\n",
       "      <td>URL</td>\n",
       "      <td>russia launches major upgrade of air defence s...</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>russia launches major upgrade of air defence s...</td>\n",
       "      <td>dmitry sudakov lyuba lulko oleg artyukov</td>\n",
       "      <td>NaN</td>\n",
       "      <td>russian air defense antiaircraft systems colle...</td>\n",
       "      <td>first and foremost it goes about antiaircraft ...</td>\n",
       "      <td>missile defense sNUM antiaircraft russian weap...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>russia launches major upgrade air defence syst...</td>\n",
       "      <td>russia launch major upgrad air defenc system m...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NUM</td>\n",
       "      <td>NUM</td>\n",
       "      <td>nytimescom</td>\n",
       "      <td>1</td>\n",
       "      <td>URL</td>\n",
       "      <td>washington  house republicans including newly ...</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>house gop to vote next week on earmarks ban</td>\n",
       "      <td>david m herszenhorn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>united states politics and government earmarks...</td>\n",
       "      <td>republican leaders called on president obama t...</td>\n",
       "      <td>times topic earmarks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nytimes</td>\n",
       "      <td>washington house republicans including newly e...</td>\n",
       "      <td>washington hous republican includ newli elect ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NUM</td>\n",
       "      <td>NUM</td>\n",
       "      <td>thepostemailcom</td>\n",
       "      <td>0</td>\n",
       "      <td>URL</td>\n",
       "      <td>freedoms for which so many have died to defend...</td>\n",
       "      <td>NUMNUMNUMtNUMNUMNUMNUM</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>kim il sung archives</td>\n",
       "      <td>jeffrey harrison jonathan david mooers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>freedoms many died defend eroded eyes sharon r...</td>\n",
       "      <td>freedom mani die defend erod eye sharon rondea...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NUM</td>\n",
       "      <td>NUM</td>\n",
       "      <td>nytimescom</td>\n",
       "      <td>1</td>\n",
       "      <td>URL</td>\n",
       "      <td>photo shuffling one foot at a time with the he...</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>jury selection begins in fraud trial of brooke...</td>\n",
       "      <td>john eligon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>astor brooke marshall anthony d frauds and swi...</td>\n",
       "      <td>anthony marshall has been charged with defraud...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nytimes</td>\n",
       "      <td>photo shuffling one foot time help oak cane so...</td>\n",
       "      <td>photo shuffl one foot time help oak cane son b...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NUM</td>\n",
       "      <td>NUM</td>\n",
       "      <td>thenewsdoctorscom</td>\n",
       "      <td>0</td>\n",
       "      <td>URL</td>\n",
       "      <td>former reagan administration white house budge...</td>\n",
       "      <td>NUMNUMNUMtNUMNUMNUMNUM</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>trump faces debt burden madness david stockman...</td>\n",
       "      <td>guest post</td>\n",
       "      <td>NaN</td>\n",
       "      <td>finacial gold news silver news precious metals...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dave kranzler alan caruba wall street for main...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>former reagan administration white house budge...</td>\n",
       "      <td>former reagan administr white hous budget dire...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0   id             domain  type  url  \\\n",
       "0        NUM  NUM    pravdareportcom     0  URL   \n",
       "1        NUM  NUM         nytimescom     1  URL   \n",
       "2        NUM  NUM    thepostemailcom     0  URL   \n",
       "3        NUM  NUM         nytimescom     1  URL   \n",
       "4        NUM  NUM  thenewsdoctorscom     0  URL   \n",
       "\n",
       "                                             content              scraped_at  \\\n",
       "0  russia launches major upgrade of air defence s...       DATE NUMNUMNUMNUM   \n",
       "1  washington  house republicans including newly ...       DATE NUMNUMNUMNUM   \n",
       "2  freedoms for which so many have died to defend...  NUMNUMNUMtNUMNUMNUMNUM   \n",
       "3  photo shuffling one foot at a time with the he...       DATE NUMNUMNUMNUM   \n",
       "4  former reagan administration white house budge...  NUMNUMNUMtNUMNUMNUMNUM   \n",
       "\n",
       "         inserted_at         updated_at  \\\n",
       "0  DATE NUMNUMNUMNUM  DATE NUMNUMNUMNUM   \n",
       "1  DATE NUMNUMNUMNUM  DATE NUMNUMNUMNUM   \n",
       "2  DATE NUMNUMNUMNUM  DATE NUMNUMNUMNUM   \n",
       "3  DATE NUMNUMNUMNUM  DATE NUMNUMNUMNUM   \n",
       "4  DATE NUMNUMNUMNUM  DATE NUMNUMNUMNUM   \n",
       "\n",
       "                                               title  \\\n",
       "0  russia launches major upgrade of air defence s...   \n",
       "1        house gop to vote next week on earmarks ban   \n",
       "2                              kim il sung archives    \n",
       "3  jury selection begins in fraud trial of brooke...   \n",
       "4  trump faces debt burden madness david stockman...   \n",
       "\n",
       "                                    authors  keywords  \\\n",
       "0  dmitry sudakov lyuba lulko oleg artyukov       NaN   \n",
       "1                       david m herszenhorn       NaN   \n",
       "2    jeffrey harrison jonathan david mooers       NaN   \n",
       "3                               john eligon       NaN   \n",
       "4                                guest post       NaN   \n",
       "\n",
       "                                       meta_keywords  \\\n",
       "0  russian air defense antiaircraft systems colle...   \n",
       "1  united states politics and government earmarks...   \n",
       "2                                                NaN   \n",
       "3  astor brooke marshall anthony d frauds and swi...   \n",
       "4  finacial gold news silver news precious metals...   \n",
       "\n",
       "                                    meta_description  \\\n",
       "0  first and foremost it goes about antiaircraft ...   \n",
       "1  republican leaders called on president obama t...   \n",
       "2                                                NaN   \n",
       "3  anthony marshall has been charged with defraud...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                                tags  summary   source  \\\n",
       "0  missile defense sNUM antiaircraft russian weap...      NaN      NaN   \n",
       "1                               times topic earmarks      NaN  nytimes   \n",
       "2                                                NaN      NaN      NaN   \n",
       "3                                                NaN      NaN  nytimes   \n",
       "4  dave kranzler alan caruba wall street for main...      NaN      NaN   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  russia launches major upgrade air defence syst...   \n",
       "1  washington house republicans including newly e...   \n",
       "2  freedoms many died defend eroded eyes sharon r...   \n",
       "3  photo shuffling one foot time help oak cane so...   \n",
       "4  former reagan administration white house budge...   \n",
       "\n",
       "                                     stemmed_content cleaned_text  \n",
       "0  russia launch major upgrad air defenc system m...          NaN  \n",
       "1  washington hous republican includ newli elect ...          NaN  \n",
       "2  freedom mani die defend erod eye sharon rondea...          NaN  \n",
       "3  photo shuffl one foot time help oak cane son b...          NaN  \n",
       "4  former reagan administr white hous budget dire...          NaN  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_scraped_combined = pd.concat([train_data, cleaned_scraped_articles], ignore_index=True)\n",
    "\n",
    "# Vis de første par rækker af den nye kombinerede DataFrame for at bekræfte, at alt ser korrekt ud\n",
    "train_scraped_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scraped_combined.to_csv('train_scraped_combined.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu prøver vi at bruge vores simple baseline \"naive bayes\" og ser om de 5767 ekstra artikler kan øge præcisionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.5863417085427136\n",
      "F1-Score: 0.4334\n"
     ]
    }
   ],
   "source": [
    "# Definerer pipeline\n",
    "baseline_pipeline = pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    MultinomialNB()\n",
    ")\n",
    "\n",
    "# Træn modellen på den nye kombinerede træningsdata\n",
    "baseline_pipeline.fit(train_scraped_combined['stemmed_content'], train_scraped_combined['type'])\n",
    "\n",
    "# Antag at 'X_val' og 'y_val' er defineret og forberedt korrekt fra din valideringsdatasæt\n",
    "\n",
    "# Brug pipeline til at forudsige på valideringsdata\n",
    "y_pred = baseline_pipeline.predict(X_val)\n",
    "\n",
    "# Beregn og udskriv præcision for valideringsdata\n",
    "val_accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation accuracy with additional reliable data: {val_accuracy}\")\n",
    "\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som vi kan se på resultatet er nøjagtigheden blevet øget med ca. 0,3% hvilket giver god mening efter som vi har tilføjet 5767 artikler til train_data som i forvejen bestod af 796000 artikler. Da det kun er 5767 artikler vi har tilføjet til train_data havde vi også forventet en meget lille forbedring i nøjagtigheden. Vi har altså kun øget træningssættet med 0,72% og dermed skulle det ikke forbedre nøjagtigheden vildt meget. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dette er vores advanced moded, efter nogle forsøg med forskellige metoder bla. under skabelsen af baseline modellen, kom vi frem til at logistic regression viste os de bedste resultater. Ligesom i baselinemodellen viste tifløjelse af domain et tegn på overfitting hvor vi nu fik 1.0 validation accuracy. Derfor tilføjer vi igen authors som en meta feature og kan se at modellen giver os en højere score gående fra fra "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "0    465516\n",
      "1    330484\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data['type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 39\u001b[0m\n\u001b[1;32m     33\u001b[0m advanced_pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[1;32m     34\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m'\u001b[39m, preprocessor),\n\u001b[1;32m     35\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m'\u001b[39m, LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m     36\u001b[0m ])\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Træn modellen på træningsdata\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m advanced_pipeline\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Forudsige på valideringsdata\u001b[39;00m\n\u001b[1;32m     42\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m advanced_pipeline\u001b[38;5;241m.\u001b[39mpredict(X_val)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py:420\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    419\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 420\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_last_step)\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1252\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1250\u001b[0m classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[1;32m   1251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_classes \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m-> 1252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1253\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis solver needs samples of at least 2 classes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1254\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the data, but the data contains only one\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1255\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m class: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1256\u001b[0m         \u001b[38;5;241m%\u001b[39m classes_[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1257\u001b[0m     )\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1260\u001b[0m     n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Antager at train_data og val_data allerede er indlæst\n",
    "\n",
    "# Erstat NaN-værdier i 'stemmed_content' og 'domain' med en tom streng\n",
    "train_data['stemmed_content'].fillna('', inplace=True)\n",
    "train_data['authors'].fillna('', inplace=True)\n",
    "\n",
    "val_data['stemmed_content'].fillna('', inplace=True)\n",
    "val_data['authors'].fillna('', inplace=True)\n",
    "\n",
    "# Anvend denne funktion på dit datasæt for at oprette en binær label\n",
    "y_train = train_data['type'].apply(map_to_binary)\n",
    "y_val = val_data['type'].apply(map_to_binary)\n",
    "\n",
    "# Forbered features\n",
    "X_train = train_data[['stemmed_content', 'authors']]\n",
    "X_val = val_data[['stemmed_content', 'authors']]\n",
    "\n",
    "# Opret en ColumnTransformer til at forarbejde de forskellige kolonner\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', TfidfVectorizer(), 'stemmed_content'),\n",
    "        ('authors', OneHotEncoder(handle_unknown='ignore'), ['authors'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Opret en pipeline med preprocessor og logistisk regression\n",
    "advanced_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Træn modellen på træningsdata\n",
    "advanced_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Forudsige på valideringsdata\n",
    "y_pred = advanced_pipeline.predict(X_val)\n",
    "\n",
    "# Beregn og udskriv præcision for valideringsdata\n",
    "val_accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "# Beregn MSE ved at sammenligne forudsagte sandsynligheder med de faktiske labels\n",
    "y_pred_probs = advanced_pipeline.predict_proba(X_val)[:, 1]\n",
    "mse_logistic = mean_squared_error(y_val, y_pred_probs)\n",
    "print(f\"LogisticRegression MSE: {mse_logistic}\")\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her bruger vi så vores baseline og advanced pipelines til at evaluaere på test settet vi opdelte i Part 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.923175879396985\n",
      "F1-score: 0.904659748553183\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_data = pd.read_csv('test_set.csv')\n",
    "test_data['stemmed_content'].fillna('', inplace=True)\n",
    "test_data['type'] = test_data['type'].apply(map_to_binary)  # Brug din funktion til at mappe til binære værdier\n",
    "\n",
    "# Forbered test features og labels\n",
    "X_test = test_data[['stemmed_content', 'authors']]  # Brug liste notation for at tilgå flere kolonner\n",
    "y_test = test_data['type']\n",
    "\n",
    "\n",
    "# Brug din pipeline til at forudsige på test sættet\n",
    "y_pred_test = baseline_pipeline.predict(X_test)\n",
    "\n",
    "# Beregn og udskriv nøjagtighed og F1-score for test sættet\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "f1 = f1_score(y_test, y_pred_test,)  # Brug 'weighted' hvis klasserne er ubalancerede\n",
    "print(f\"F1-score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9380703517587939\n",
      "F1-score: 0.925234778810454\n"
     ]
    }
   ],
   "source": [
    "# Forbered test features og labels\n",
    "X_test = test_data[['stemmed_content', 'authors']]  # Brug liste notation for at tilgå flere kolonner\n",
    "y_test = test_data['type']\n",
    "\n",
    "\n",
    "# Brug din pipeline til at forudsige på test sættet\n",
    "y_pred_test = advanced_pipeline.predict(X_test)\n",
    "\n",
    "# Beregn og udskriv nøjagtighed og F1-score for test sættet\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "f1 = f1_score(y_test, y_pred_test,)  # Brug 'weighted' hvis klasserne er ubalancerede\n",
    "print(f\"F1-score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
