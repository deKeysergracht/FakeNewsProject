{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I part 2 starter vi ud med at indlæse vores 'train_set.csv', 'validation_set.csv' og 'test_set.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_set.csv')\n",
    "val_data = pd.read_csv('validation_set.csv')\n",
    "test_data = pd.read_csv('test_set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################\n",
    "# Task 0\n",
    "################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For at udføre task 0, beskriv her hvorfor vi har valgt præcist political og reliable til at være reliable og resten fake??????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####################\n",
    "# Task 1\n",
    "####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi vil nu konvertere 'type' kolonnen til en binær klassificering, hvor 1 er lig med alle de reliable news og 0 er lig med alle de fake news. Samtidig med dette opretter vi en baseline model, som vi har valgt til at være naive bayes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8293869346733669\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Indlæs træningsdata\n",
    "train_data = pd.read_csv('train_set.csv')\n",
    "\n",
    "# Erstat NaN-værdier i 'stemmed_content' med en tom streng\n",
    "train_data['stemmed_content'].fillna('', inplace=True)\n",
    "\n",
    "# Definer dine kategorier\n",
    "reliable_categories = ['reliable', 'political']\n",
    "fake_categories = ['fake', 'bias', 'conspiracy', 'hate', 'junksci', 'rumor', 'unreliable', 'satire']\n",
    "\n",
    "# Opret en funktion, der omdanner 'type' til binære værdier\n",
    "def map_to_binary(article_type):\n",
    "    if article_type in reliable_categories:\n",
    "        return 1  # 'reliable'\n",
    "    elif article_type in fake_categories:\n",
    "        return 0  # 'fake'\n",
    "    else:\n",
    "        return 0  # Sikrer, at funktionen returnerer 0 for alle andre tilfælde, hvilket eliminerer risikoen for NaN-værdier\n",
    "\n",
    "# Opdater 'type' kolonnen i 'train_data' med de binære værdier\n",
    "train_data['type'] = train_data['type'].apply(map_to_binary)\n",
    "\n",
    "# Nu er 'train_data['type']' klar til brug som 'y_train'\n",
    "y_train = train_data['type']\n",
    "X_train = train_data['stemmed_content']\n",
    "\n",
    "# Opret en pipeline med TF-IDF og Naive Bayes\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Træn modellen på træningsdata\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Her er den del hvor vi indlæser valideringsdata og forudsiger\n",
    "# Antager at du har en 'val_set.csv' som valideringsdatasæt\n",
    "val_data = pd.read_csv('validation_set.csv')\n",
    "val_data['stemmed_content'].fillna('', inplace=True)\n",
    "\n",
    "# Opdater 'type' kolonnen i 'val_data' med de binære værdier\n",
    "val_data['type'] = val_data['type'].apply(map_to_binary)\n",
    "\n",
    "X_val = val_data['stemmed_content']\n",
    "y_val = val_data['type']\n",
    "\n",
    "# Brug pipelinen til at forudsige på valideringsdata\n",
    "y_pred = pipeline.predict(X_val)\n",
    "\n",
    "# Beregn og udskriv præcision for valideringsdata\n",
    "val_accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation accuracy: {val_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beregner også F1_Score, Recall og Precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8771\n",
      "Recall: 0.6841\n",
      "F1-Score: 0.7686\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Beregn præcision, recall og F1-score\n",
    "precision = precision_score(y_val, y_pred)\n",
    "recall = recall_score(y_val, y_pred)\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#############################\n",
    "# Task 2\n",
    "#############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iforhold til meta data overvejede vi domain men i og med vi tidligere fandt ud af at hver domain kun udgiver 1 type artikel mener vi at det vil \"overfitte\" vores model, hvor vores model bare genkender domain istedet for at forudsige. Vi valgte derfor i stedet at prøve med authors og for at om dette spillede nogen rolle. Det viser sig at accuracy stiger med ca. 10%, med author som meta-feature rammer vi nu en accuracy på 92,2 %. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.9220603015075377\n",
      "F1-Score: 0.8994\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Indlæs træningsdata\n",
    "train_data = pd.read_csv('train_set.csv')\n",
    "train_data['stemmed_content'].fillna('', inplace=True)\n",
    "train_data['authors'].fillna('', inplace=True)\n",
    "\n",
    "# Indlæs valideringsdata\n",
    "val_data = pd.read_csv('validation_set.csv')\n",
    "val_data['stemmed_content'].fillna('', inplace=True)\n",
    "val_data['authors'].fillna('', inplace=True)\n",
    "\n",
    "# Omdan 'type' til binære værdier\n",
    "train_data['type'] = train_data['type'].apply(map_to_binary)\n",
    "val_data['type'] = val_data['type'].apply(map_to_binary)\n",
    "\n",
    "# Forbered features og labels\n",
    "X_train = train_data[['stemmed_content', 'authors']]\n",
    "y_train = train_data['type']\n",
    "X_val = val_data[['stemmed_content', 'authors']]\n",
    "y_val = val_data['type']\n",
    "\n",
    "# Opret en ColumnTransformer til at forarbejde de forskellige kolonner\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', TfidfVectorizer(), 'stemmed_content'),\n",
    "        ('authors', OneHotEncoder(handle_unknown='ignore'), ['authors'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Opret en pipeline med preprocessor og Naive Bayes\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Træn modellen på træningsdata\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Forudsige på valideringsdata\n",
    "y_pred = pipeline.predict(X_val)\n",
    "\n",
    "# Beregn og udskriv præcision for valideringsdata\n",
    "val_accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######################\n",
    "# task 3\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeg starter med at indlæse filen fra exercise 2 - scraped_articles.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_articles = pd.read_csv('scraped_articles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da disse artikler ikke er blevet renset eller lavet stemming på endnu, vil vi starte med at gøre dette:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                            cleaned_text\n",
       "0     we know whats coming east ukraine braces for r...\n",
       "1     in eastern ukraine the tide of this war hasnt ...\n",
       "2     in order to preserve life and encirclement i h...\n",
       "3     fighting has been raging in ukraine for two ye...\n",
       "4     deadly explosions have rocked ukraines souther...\n",
       "...                                                 ...\n",
       "5762  indias capital delhi has banned motorbike taxi...\n",
       "5763  two years ago indian prime minister narendra m...\n",
       "5764  the discovery of two charred bodies in a burnt...\n",
       "5765  the judges of masterchef india have been criti...\n",
       "5766  ruchelle barrie remembers feeling out of place...\n",
       "\n",
       "[5767 rows x 1 columns]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):  # Tjekker for NaN værdier og konverterer dem til en tom streng\n",
    "        return ''\n",
    "    text = str(text).lower()  # Sikrer at tekst er en streng og konverterer til små bogstaver\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Erstatter al hvidplads med et enkelt mellemrum\n",
    "    text = re.sub(r'\\b\\d{4}-\\d{2}-\\d{2}\\b', '<DATE>', text)  # Erstatter datoer med pladsholder\n",
    "    text = re.sub(r'\\d+', '<NUM>', text)  # Erstatter tal med pladsholder\n",
    "    text = re.sub(r'\\S+@\\S+', '<EMAIL>', text)  # Erstatter e-mails med pladsholder\n",
    "    text = re.sub(r'https?://\\S+', '<URL>', text)  # Erstatter URL'er med pladsholder\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Fjerner tegnsætning\n",
    "    return text\n",
    "\n",
    "# Path to the scraped articles CSV file\n",
    "csv_file_path = 'scraped_articles.csv'  # Erstat med din faktiske filsti\n",
    "\n",
    "# Load the scraped articles data from the CSV file\n",
    "scraped_articles = pd.read_csv(csv_file_path, dtype=str)\n",
    "\n",
    "# Apply the cleaning function to the 'text' column which contains the article text\n",
    "scraped_articles['cleaned_text'] = scraped_articles['text'].apply(clean_text)\n",
    "\n",
    "# Keeping only the 'cleaned_text' column for further processing\n",
    "cleaned_scraped_articles = scraped_articles[['cleaned_text']]\n",
    "\n",
    "# Optional: Save the cleaned data to a new CSV file if needed\n",
    "# cleaned_scraped_articles.to_csv('path_to_your_file/cleaned_scraped_articles.csv', index=False)\n",
    "cleaned_scraped_articles.head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/bruger/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/bruger/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/var/folders/78/tggs5x_100l2bhsl5ydp24mh0000gn/T/ipykernel_38027/1439455173.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_scraped_articles.loc[:, 'stemmed_content'] = cleaned_scraped_articles['cleaned_text'].apply(remove_stopwords_and_stem)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>stemmed_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>we know whats coming east ukraine braces for r...</td>\n",
       "      <td>know what come east ukrain brace russian advan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in eastern ukraine the tide of this war hasnt ...</td>\n",
       "      <td>eastern ukrain tide war hasnt chang come fast ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in order to preserve life and encirclement i h...</td>\n",
       "      <td>order preserv life encircl withdrawn unit avdi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fighting has been raging in ukraine for two ye...</td>\n",
       "      <td>fight rage ukrain two year sinc russia invas m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deadly explosions have rocked ukraines souther...</td>\n",
       "      <td>deadli explos rock ukrain southern port citi o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_text  \\\n",
       "0  we know whats coming east ukraine braces for r...   \n",
       "1  in eastern ukraine the tide of this war hasnt ...   \n",
       "2  in order to preserve life and encirclement i h...   \n",
       "3  fighting has been raging in ukraine for two ye...   \n",
       "4  deadly explosions have rocked ukraines souther...   \n",
       "\n",
       "                                     stemmed_content  \n",
       "0  know what come east ukrain brace russian advan...  \n",
       "1  eastern ukrain tide war hasnt chang come fast ...  \n",
       "2  order preserv life encircl withdrawn unit avdi...  \n",
       "3  fight rage ukrain two year sinc russia invas m...  \n",
       "4  deadli explos rock ukrain southern port citi o...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download stopwords from NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def remove_stopwords_and_stem(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    # Remove stop words and stem each word\n",
    "    return ' '.join([stemmer.stem(word) for word in words if word not in stop_words])\n",
    "\n",
    "# Apply the function to remove stop words and stem the words in the 'cleaned_text' column\n",
    "# Brug .loc til at undgå SettingWithCopyWarning\n",
    "cleaned_scraped_articles.loc[:, 'stemmed_content'] = cleaned_scraped_articles['cleaned_text'].apply(remove_stopwords_and_stem)\n",
    "\n",
    "# Vis de første par rækker for at bekræfte ændringerne\n",
    "cleaned_scraped_articles.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu har vi altså renset og stemmed vores scraped_articles.csv og lavet en ny kolonne i dataen, som hedder cleaned_stemmed_text. Det næste vi vil gøre er at oprette en kolonne 'type' som fortæller om artiklen er reliable eller fake. Her vil vi vurdere alle artiklerne til at være reliable, da de kommer fra bbc. Dette vil altså sige at der kommer til at stå et 1 til under 'type' for alle artiklerne, da det betyder reliable i vores binære klassificering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/78/tggs5x_100l2bhsl5ydp24mh0000gn/T/ipykernel_38027/2909232008.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_scraped_articles['type'] = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>stemmed_content</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>we know whats coming east ukraine braces for r...</td>\n",
       "      <td>know what come east ukrain brace russian advan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in eastern ukraine the tide of this war hasnt ...</td>\n",
       "      <td>eastern ukrain tide war hasnt chang come fast ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in order to preserve life and encirclement i h...</td>\n",
       "      <td>order preserv life encircl withdrawn unit avdi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fighting has been raging in ukraine for two ye...</td>\n",
       "      <td>fight rage ukrain two year sinc russia invas m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deadly explosions have rocked ukraines souther...</td>\n",
       "      <td>deadli explos rock ukrain southern port citi o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5762</th>\n",
       "      <td>indias capital delhi has banned motorbike taxi...</td>\n",
       "      <td>india capit delhi ban motorbik taxi road deal ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5763</th>\n",
       "      <td>two years ago indian prime minister narendra m...</td>\n",
       "      <td>two year ago indian prime minist narendra modi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5764</th>\n",
       "      <td>the discovery of two charred bodies in a burnt...</td>\n",
       "      <td>discoveri two char bodi burnt vehicl india har...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5765</th>\n",
       "      <td>the judges of masterchef india have been criti...</td>\n",
       "      <td>judg masterchef india criticis social media le...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5766</th>\n",
       "      <td>ruchelle barrie remembers feeling out of place...</td>\n",
       "      <td>ruchel barri rememb feel place school name fre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5767 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           cleaned_text  \\\n",
       "0     we know whats coming east ukraine braces for r...   \n",
       "1     in eastern ukraine the tide of this war hasnt ...   \n",
       "2     in order to preserve life and encirclement i h...   \n",
       "3     fighting has been raging in ukraine for two ye...   \n",
       "4     deadly explosions have rocked ukraines souther...   \n",
       "...                                                 ...   \n",
       "5762  indias capital delhi has banned motorbike taxi...   \n",
       "5763  two years ago indian prime minister narendra m...   \n",
       "5764  the discovery of two charred bodies in a burnt...   \n",
       "5765  the judges of masterchef india have been criti...   \n",
       "5766  ruchelle barrie remembers feeling out of place...   \n",
       "\n",
       "                                        stemmed_content  type  \n",
       "0     know what come east ukrain brace russian advan...     1  \n",
       "1     eastern ukrain tide war hasnt chang come fast ...     1  \n",
       "2     order preserv life encircl withdrawn unit avdi...     1  \n",
       "3     fight rage ukrain two year sinc russia invas m...     1  \n",
       "4     deadli explos rock ukrain southern port citi o...     1  \n",
       "...                                                 ...   ...  \n",
       "5762  india capit delhi ban motorbik taxi road deal ...     1  \n",
       "5763  two year ago indian prime minist narendra modi...     1  \n",
       "5764  discoveri two char bodi burnt vehicl india har...     1  \n",
       "5765  judg masterchef india criticis social media le...     1  \n",
       "5766  ruchel barri rememb feel place school name fre...     1  \n",
       "\n",
       "[5767 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_scraped_articles['type'] = 1\n",
    "\n",
    "cleaned_scraped_articles.to_csv('cleaned_scraped_articled')\n",
    "\n",
    "cleaned_scraped_articles\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu hvor vi har en kolonne med stemmed_content hvor alle er sat til reliable under type, vil vi sammensætte vores train_data og cleaned_scraped_articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>domain</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>scraped_at</th>\n",
       "      <th>inserted_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>meta_keywords</th>\n",
       "      <th>meta_description</th>\n",
       "      <th>tags</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stemmed_content</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NUM</td>\n",
       "      <td>NUMNUM</td>\n",
       "      <td>abovetopsecretcom</td>\n",
       "      <td>0</td>\n",
       "      <td>URL</td>\n",
       "      <td>geoengineering cost analysis you lolyes seen ...</td>\n",
       "      <td>NUMNUMNUMtNUMNUMNUMNUM</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>possible weather mod setup page NUM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>geoengineering cost analysis lolyes seen docum...</td>\n",
       "      <td>geoengin cost analysi loly seen documentwhat t...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NUM</td>\n",
       "      <td>NUMNUM</td>\n",
       "      <td>thelibertybeaconcom</td>\n",
       "      <td>0</td>\n",
       "      <td>URL</td>\n",
       "      <td>lawNUM philadelphia february NUM NUM NUMNUM pm...</td>\n",
       "      <td>NUMNUMNUMtNUMNUMNUMNUM</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>merck sued in philly over shingles vaccine inj...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>natural immunity chickenpox merck lawsuit vacc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lawNUM philadelphia february NUM NUM NUMNUM pm...</td>\n",
       "      <td>lawnum philadelphia februari num num numnum pm...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NUM</td>\n",
       "      <td>NUM</td>\n",
       "      <td>newswithviewscom</td>\n",
       "      <td>0</td>\n",
       "      <td>URL</td>\n",
       "      <td>additional titles our right to attack iraq by ...</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>devvy kidd  new gun bill in congress more unco...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>additional titles right attack iraq devvy kidd...</td>\n",
       "      <td>addit titl right attack iraq devvi kidd februa...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NUM</td>\n",
       "      <td>NUM</td>\n",
       "      <td>nytimescom</td>\n",
       "      <td>1</td>\n",
       "      <td>URL</td>\n",
       "      <td>i dont know who takes him said an eastern conf...</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>multiple nicknames dwindling openings</td>\n",
       "      <td>howard beck</td>\n",
       "      <td>NaN</td>\n",
       "      <td>oneal shaquille basketball free agents sports ...</td>\n",
       "      <td>shaquille oneal who once rumbled across the co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nytimes</td>\n",
       "      <td>dont know takes said eastern conference scout ...</td>\n",
       "      <td>dont know take said eastern confer scout cite ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NUM</td>\n",
       "      <td>NUM</td>\n",
       "      <td>nationalreviewcom</td>\n",
       "      <td>1</td>\n",
       "      <td>URL</td>\n",
       "      <td>plus one article on google plus thanks to ali ...</td>\n",
       "      <td>NUMNUMNUMtNUMNUMNUMNUM</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>DATE NUMNUMNUMNUM</td>\n",
       "      <td>iran news round up</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>national review national review online article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>plus one article google plus thanks ali alfone...</td>\n",
       "      <td>plu one articl googl plu thank ali alfoneh ass...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0      id               domain  type  url  \\\n",
       "0        NUM  NUMNUM    abovetopsecretcom     0  URL   \n",
       "1        NUM  NUMNUM  thelibertybeaconcom     0  URL   \n",
       "2        NUM     NUM     newswithviewscom     0  URL   \n",
       "3        NUM     NUM           nytimescom     1  URL   \n",
       "4        NUM     NUM    nationalreviewcom     1  URL   \n",
       "\n",
       "                                             content              scraped_at  \\\n",
       "0   geoengineering cost analysis you lolyes seen ...  NUMNUMNUMtNUMNUMNUMNUM   \n",
       "1  lawNUM philadelphia february NUM NUM NUMNUM pm...  NUMNUMNUMtNUMNUMNUMNUM   \n",
       "2  additional titles our right to attack iraq by ...       DATE NUMNUMNUMNUM   \n",
       "3  i dont know who takes him said an eastern conf...       DATE NUMNUMNUMNUM   \n",
       "4  plus one article on google plus thanks to ali ...  NUMNUMNUMtNUMNUMNUMNUM   \n",
       "\n",
       "         inserted_at         updated_at  \\\n",
       "0  DATE NUMNUMNUMNUM  DATE NUMNUMNUMNUM   \n",
       "1  DATE NUMNUMNUMNUM  DATE NUMNUMNUMNUM   \n",
       "2  DATE NUMNUMNUMNUM  DATE NUMNUMNUMNUM   \n",
       "3  DATE NUMNUMNUMNUM  DATE NUMNUMNUMNUM   \n",
       "4  DATE NUMNUMNUMNUM  DATE NUMNUMNUMNUM   \n",
       "\n",
       "                                               title      authors  keywords  \\\n",
       "0                possible weather mod setup page NUM          NaN       NaN   \n",
       "1  merck sued in philly over shingles vaccine inj...          NaN       NaN   \n",
       "2  devvy kidd  new gun bill in congress more unco...          NaN       NaN   \n",
       "3              multiple nicknames dwindling openings  howard beck       NaN   \n",
       "4                                 iran news round up          NaN       NaN   \n",
       "\n",
       "                                       meta_keywords  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3  oneal shaquille basketball free agents sports ...   \n",
       "4     national review national review online article   \n",
       "\n",
       "                                    meta_description  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3  shaquille oneal who once rumbled across the co...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                                tags  summary   source  \\\n",
       "0                                                NaN      NaN      NaN   \n",
       "1  natural immunity chickenpox merck lawsuit vacc...      NaN      NaN   \n",
       "2                                                NaN      NaN      NaN   \n",
       "3                                                NaN      NaN  nytimes   \n",
       "4                                                NaN      NaN      NaN   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  geoengineering cost analysis lolyes seen docum...   \n",
       "1  lawNUM philadelphia february NUM NUM NUMNUM pm...   \n",
       "2  additional titles right attack iraq devvy kidd...   \n",
       "3  dont know takes said eastern conference scout ...   \n",
       "4  plus one article google plus thanks ali alfone...   \n",
       "\n",
       "                                     stemmed_content cleaned_text  \n",
       "0  geoengin cost analysi loly seen documentwhat t...          NaN  \n",
       "1  lawnum philadelphia februari num num numnum pm...          NaN  \n",
       "2  addit titl right attack iraq devvi kidd februa...          NaN  \n",
       "3  dont know take said eastern confer scout cite ...          NaN  \n",
       "4  plu one articl googl plu thank ali alfoneh ass...          NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_scraped_combined = pd.concat([train_data, cleaned_scraped_articles], ignore_index=True)\n",
    "\n",
    "# Vis de første par rækker af den nye kombinerede DataFrame for at bekræfte, at alt ser korrekt ud\n",
    "train_scraped_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scraped_combined.to_csv('train_scraped_combined.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu prøver vi at bruge vores simple baseline \"naive bayes\" og ser om de 5767 ekstra artikler kan øge præcisionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy with additional reliable data: 0.8320904522613065\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Træn modellen på den nye kombinerede træningsdata\n",
    "pipeline.fit(train_scraped_combined['stemmed_content'], train_scraped_combined['type'])\n",
    "\n",
    "# Valideringsdatasættet forbliver det samme som før\n",
    "# Husk at sikre, at 'stemmed_content' og 'type' kolonnerne er forberedt som før\n",
    "\n",
    "# Brug pipelinen til at forudsige på valideringsdata\n",
    "y_pred = pipeline.predict(X_val)\n",
    "\n",
    "# Beregn og udskriv præcision for valideringsdata\n",
    "val_accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation accuracy with additional reliable data: {val_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som vi kan se på resultatet er nøjagtigheden blevet øget med ca. 0,3% hvilket giver god mening efter som vi har tilføjet 5767 artikler til train_data som i forvejen bestod af 796000 artikler. Da det kun er 5767 artikler vi har tilføjet til train_data havde vi også forventet en meget lille forbedring i nøjagtigheden. Vi har altså kun øget træningssættet med 0,72% og dermed skulle det ikke forbedre nøjagtigheden vildt meget. Ud fra dette resultat, har vi besluttet os for ikke at arbejde videre med dette datasæt, da forøgelsen af nøjagtighed er så lille. Men ud fra resultatet, kan man konkluderer at mere data at træne modellen med er lig med bedre nøjagtighed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####################\n",
    "# Part 3\n",
    "#####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bruger vores advanced model her, som resulterede i en logistic regression, hvor vi har tilføjet authors som meta-feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.9423316582914573\n",
      "LogisticRegression MSE: 0.043330744473855855\n",
      "F1-Score: 0.9284\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Antager at train_data og val_data allerede er indlæst\n",
    "\n",
    "train_data = pd.read_csv('train_set.csv')\n",
    "val_data = pd.read_csv('validation_set.csv')\n",
    "\n",
    "# Erstat NaN-værdier separat i hver kolonne\n",
    "train_data['stemmed_content'].fillna('', inplace=True)\n",
    "train_data['authors'].fillna('', inplace=True)\n",
    "val_data['stemmed_content'].fillna('', inplace=True)\n",
    "val_data['authors'].fillna('', inplace=True)\n",
    "\n",
    "\n",
    "# Anvend denne funktion på dit datasæt for at oprette en binær label\n",
    "y_train = train_data['type'].apply(map_to_binary)\n",
    "y_val = val_data['type'].apply(map_to_binary)\n",
    "\n",
    "# Forbered features\n",
    "X_train = train_data[['stemmed_content', 'authors']]\n",
    "X_val = val_data[['stemmed_content', 'authors']]\n",
    "\n",
    "# Opret en ColumnTransformer til at forarbejde de forskellige kolonner\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', TfidfVectorizer(), 'stemmed_content'),\n",
    "        ('authors', OneHotEncoder(handle_unknown='ignore'), ['authors'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Opret en pipeline med preprocessor og logistisk regression\n",
    "advanced_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Træn modellen på træningsdata\n",
    "advanced_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Forudsige på valideringsdata\n",
    "y_pred = advanced_pipeline.predict(X_val)\n",
    "\n",
    "# Beregn og udskriv præcision for valideringsdata\n",
    "val_accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "# Beregn MSE ved at sammenligne forudsagte sandsynligheder med de faktiske labels\n",
    "y_pred_probs = advanced_pipeline.predict_proba(X_val)[:, 1]\n",
    "mse_logistic = mean_squared_error(y_val, y_pred_probs)\n",
    "print(f\"LogisticRegression MSE: {mse_logistic}\")\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9561\n",
      "Recall: 0.9022\n",
      "F1-Score: 0.9284\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Beregn præcision, recall og F1-score\n",
    "precision = precision_score(y_val, y_pred)\n",
    "recall = recall_score(y_val, y_pred)\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#################\n",
    "# Part 4\n",
    "#################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################\n",
    "# Task 1\n",
    "################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her prøver vi at køre vores advanced pipeline på test sættet som modellen altså ikke er trænet på i forvejen. Resultatet viser os at den næsten fungerer ligeså godt som den gjorde på vores validation sæt, som den allerede var trænet på, hvilket tyder på at modellen virker rigtig godt. Det er et godt resultat, da formålet er at den skal kunne virke godt på ukendte artikler og kunne vurdere dem til enten fake eller reliable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9405929648241206\n",
      "F1-score: 0.9261300441145228\n",
      "LogisticRegression MSE: 0.0594070351758794\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv('test_set.csv')\n",
    "test_data['stemmed_content'].fillna('', inplace=True)\n",
    "test_data['authors'].fillna('', inplace=True)\n",
    "test_data['type'] = test_data['type'].apply(map_to_binary)\n",
    "\n",
    "X_test = test_data[['stemmed_content', 'authors']]  # Brug liste notation for at tilgå flere kolonner\n",
    "y_test = test_data['type']\n",
    "\n",
    "y_pred_test = advanced_pipeline.predict(X_test)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "f1 = f1_score(y_test, y_pred_test,)  # Brug 'weighted' hvis klasserne er ubalancerede\n",
    "print(f\"F1-score: {f1}\")\n",
    "\n",
    "y_pred_probs_test = advanced_pipeline.predict_proba(X_test)[:, 1]\n",
    "mse_logistic = mean_squared_error(y_test, y_pred_test)\n",
    "print(f\"LogisticRegression MSE: {mse_logistic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9206130653266331\n",
      "F1-score: 0.8974834849645041\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = pipeline.predict(X_test)\n",
    "\n",
    "# Beregn og udskriv nøjagtighed og F1-score for test sættet\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "f1 = f1_score(y_test, y_pred_test,)  # Brug 'weighted' hvis klasserne er ubalancerede\n",
    "print(f\"F1-score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi opdager lidt samme resultat med vores baseline model, her ser vi altså resultatet er ret højt, hvilket er fordi vi har authors tilføjet som meta-festure her også. Vores forrige resultat fra vores baseline model med authors som meta-feature på vores validition sæt var: Validation accuracy: 0.9220603015075377\n",
    "F1-Score: 0.8994 hvilket er meget tæt på dette resultatet, men stadig en lille smule bedre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############\n",
    "# Task 2\n",
    "###############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi starter ud med at indlæse de 3 datasæt som vi skal arbejde med her, altså: train.tsv, valid.tsv og test.tsv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_liar = pd.read_csv('train.tsv', sep='\\t')\n",
    "valid_data_liar = pd.read_csv('valid.tsv', sep='\\t')\n",
    "test_data_liar = pd.read_csv('test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2635.json</th>\n",
       "      <th>false</th>\n",
       "      <th>Says the Annies List political group supports third-trimester abortions on demand.</th>\n",
       "      <th>abortion</th>\n",
       "      <th>dwayne-bohac</th>\n",
       "      <th>State representative</th>\n",
       "      <th>Texas</th>\n",
       "      <th>republican</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>a mailer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10540.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>energy,history,job-accomplishments</td>\n",
       "      <td>scott-surovell</td>\n",
       "      <td>State delegate</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>democrat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a floor speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "      <td>foreign-policy</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>President</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>democrat</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Denver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1123.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "      <td>health-care</td>\n",
       "      <td>blog-posting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>a news release</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9028.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "      <td>economy,jobs</td>\n",
       "      <td>charlie-crist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Florida</td>\n",
       "      <td>democrat</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>an interview on CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12465.json</td>\n",
       "      <td>true</td>\n",
       "      <td>The Chicago Bears have had more starting quart...</td>\n",
       "      <td>education</td>\n",
       "      <td>robin-vos</td>\n",
       "      <td>Wisconsin Assembly speaker</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>a an online opinion-piece</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    2635.json        false  \\\n",
       "0  10540.json    half-true   \n",
       "1    324.json  mostly-true   \n",
       "2   1123.json        false   \n",
       "3   9028.json    half-true   \n",
       "4  12465.json         true   \n",
       "\n",
       "  Says the Annies List political group supports third-trimester abortions on demand.  \\\n",
       "0  When did the decline of coal start? It started...                                   \n",
       "1  Hillary Clinton agrees with John McCain \"by vo...                                   \n",
       "2  Health care reform legislation is likely to ma...                                   \n",
       "3  The economic turnaround started at the end of ...                                   \n",
       "4  The Chicago Bears have had more starting quart...                                   \n",
       "\n",
       "                             abortion    dwayne-bohac  \\\n",
       "0  energy,history,job-accomplishments  scott-surovell   \n",
       "1                      foreign-policy    barack-obama   \n",
       "2                         health-care    blog-posting   \n",
       "3                        economy,jobs   charlie-crist   \n",
       "4                           education       robin-vos   \n",
       "\n",
       "         State representative      Texas  republican     0     1    0.1  \\\n",
       "0              State delegate   Virginia    democrat   0.0   0.0    1.0   \n",
       "1                   President   Illinois    democrat  70.0  71.0  160.0   \n",
       "2                         NaN        NaN        none   7.0  19.0    3.0   \n",
       "3                         NaN    Florida    democrat  15.0   9.0   20.0   \n",
       "4  Wisconsin Assembly speaker  Wisconsin  republican   0.0   3.0    2.0   \n",
       "\n",
       "     0.2   0.3                   a mailer  \n",
       "0    1.0   0.0            a floor speech.  \n",
       "1  163.0   9.0                     Denver  \n",
       "2    5.0  44.0             a news release  \n",
       "3   19.0   2.0        an interview on CNN  \n",
       "4    5.0   1.0  a an online opinion-piece  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_liar.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som jeg kan se på det indlæste datasæt har kolonnerne ikke nogle navne, derfor vil vi starte med at navngivne dem ud fra hvad Readme siger at hver kolonne angiver. Vi antager dog at \"Label\" kolonnen svarer til \"Type\" kolonnen i FakeNewsCorpus og derfor kalder vi denne for \"type\" i stedet for \"Label\", derudover ligger vi mærke til at speaker også skal ændres til authors for at matche FakeNewsCorpus. Da der ikke er nogen direkte 'content' kolonne, vil jeg bruge 'statement' som 'content', så der også er noget der matcher 'content' kolonnen fra FakeNewsCorpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>type</th>\n",
       "      <th>stemmed_content</th>\n",
       "      <th>subjects</th>\n",
       "      <th>authors</th>\n",
       "      <th>speaker's Job Title</th>\n",
       "      <th>state Info</th>\n",
       "      <th>party Affiliation</th>\n",
       "      <th>barely True Counts</th>\n",
       "      <th>false Counts</th>\n",
       "      <th>half True Counts</th>\n",
       "      <th>mostly True Counts</th>\n",
       "      <th>pants on Fire Counts</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2635.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>dwayne-bohac</td>\n",
       "      <td>State representative</td>\n",
       "      <td>Texas</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a mailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10540.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>energy,history,job-accomplishments</td>\n",
       "      <td>scott-surovell</td>\n",
       "      <td>State delegate</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>democrat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a floor speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>324.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "      <td>foreign-policy</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>President</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>democrat</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Denver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1123.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "      <td>health-care</td>\n",
       "      <td>blog-posting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>a news release</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9028.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "      <td>economy,jobs</td>\n",
       "      <td>charlie-crist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Florida</td>\n",
       "      <td>democrat</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>an interview on CNN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID         type                                    stemmed_content  \\\n",
       "0   2635.json        false  Says the Annies List political group supports ...   \n",
       "1  10540.json    half-true  When did the decline of coal start? It started...   \n",
       "2    324.json  mostly-true  Hillary Clinton agrees with John McCain \"by vo...   \n",
       "3   1123.json        false  Health care reform legislation is likely to ma...   \n",
       "4   9028.json    half-true  The economic turnaround started at the end of ...   \n",
       "\n",
       "                             subjects         authors   speaker's Job Title  \\\n",
       "0                            abortion    dwayne-bohac  State representative   \n",
       "1  energy,history,job-accomplishments  scott-surovell        State delegate   \n",
       "2                      foreign-policy    barack-obama             President   \n",
       "3                         health-care    blog-posting                   NaN   \n",
       "4                        economy,jobs   charlie-crist                   NaN   \n",
       "\n",
       "  state Info party Affiliation  barely True Counts  false Counts  \\\n",
       "0      Texas        republican                 0.0           1.0   \n",
       "1   Virginia          democrat                 0.0           0.0   \n",
       "2   Illinois          democrat                70.0          71.0   \n",
       "3        NaN              none                 7.0          19.0   \n",
       "4    Florida          democrat                15.0           9.0   \n",
       "\n",
       "   half True Counts  mostly True Counts  pants on Fire Counts  \\\n",
       "0               0.0                 0.0                   0.0   \n",
       "1               1.0                 1.0                   0.0   \n",
       "2             160.0               163.0                   9.0   \n",
       "3               3.0                 5.0                  44.0   \n",
       "4              20.0                19.0                   2.0   \n",
       "\n",
       "               context  \n",
       "0             a mailer  \n",
       "1      a floor speech.  \n",
       "2               Denver  \n",
       "3       a news release  \n",
       "4  an interview on CNN  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = [\n",
    "    'ID', 'type', 'stemmed_content', 'subjects', 'authors',\n",
    "    'speaker\\'s Job Title', 'state Info', 'party Affiliation',\n",
    "    'barely True Counts', 'false Counts', 'half True Counts',\n",
    "    'mostly True Counts', 'pants on Fire Counts', 'context'\n",
    "]\n",
    "\n",
    "# Indlæs dine data\n",
    "train_data_liar = pd.read_csv('train.tsv', delimiter='\\t', header=None, names=column_names)\n",
    "valid_data_liar = pd.read_csv('valid.tsv', delimiter='\\t', header=None, names=column_names)\n",
    "test_data_liar = pd.read_csv('test.tsv', delimiter='\\t', header=None, names=column_names)\n",
    "\n",
    "train_data_liar.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da vores model fra FakeNewsCorpus er trænet på en binær klassificering under 'type', vil vi her starte med at gøre 'type' kolonnen for vores Liar datasæt til en binær klassificering, hvor 1 = true og 0 = false. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['false' 'half-true' 'mostly-true' 'true' 'barely-true' 'pants-fire']\n"
     ]
    }
   ],
   "source": [
    "unique_types = train_data_liar['type'].unique()\n",
    "print(unique_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Når vi kigger på værdierne under 'type' kolonnenn, vælger vi at klassificere 'true' og 'mostly-true' som reliable og resten som fake. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "0    6602\n",
      "1    3638\n",
      "Name: count, dtype: int64\n",
      "type\n",
      "0    864\n",
      "1    420\n",
      "Name: count, dtype: int64\n",
      "type\n",
      "0    818\n",
      "1    449\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def label_to_binary(label):\n",
    "    if label in ['true', 'mostly-true']:\n",
    "        return 1  # 'reliable'\n",
    "    else:\n",
    "        return 0  # 'fake'\n",
    "\n",
    "# Opdater 'type' kolonnen med de binære værdier\n",
    "train_data_liar['type'] = train_data_liar['type'].apply(label_to_binary)\n",
    "valid_data_liar['type'] = valid_data_liar['type'].apply(label_to_binary)\n",
    "test_data_liar['type'] = test_data_liar['type'].apply(label_to_binary)\n",
    "\n",
    "# Tjek om konverteringen er korrekt ved at tælle værdierne\n",
    "print(train_data_liar['type'].value_counts())\n",
    "print(valid_data_liar['type'].value_counts())\n",
    "print(test_data_liar['type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy on LIAR dataset with simple model: 0.6314127861089187\n"
     ]
    }
   ],
   "source": [
    "X_test_liar = test_data_liar[['stemmed_content', 'authors']]\n",
    "y_test_liar = test_data_liar['type']\n",
    "\n",
    "y_pred_test_liar = pipeline.predict(X_test_liar)\n",
    "test_accuracy_liar = accuracy_score(y_test_liar, y_pred_test_liar)\n",
    "print(f\"Test accuracy on LIAR dataset with simple model: {test_accuracy_liar}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy on LIAR dataset with advanced model: 0.6243093922651933\n"
     ]
    }
   ],
   "source": [
    "X_test_liar_advanved = test_data_liar[['stemmed_content', 'authors']]\n",
    "y_test_liar_advanced = test_data_liar['type']\n",
    "\n",
    "y_pred_test_liar_advanced = advanced_pipeline.predict(X_test_liar)\n",
    "test_accuracy_liar = accuracy_score(y_test_liar, y_pred_test_liar_advanced)\n",
    "print(f\"Test accuracy on LIAR dataset with advanced model: {test_accuracy_liar}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
